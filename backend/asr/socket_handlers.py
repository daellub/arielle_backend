# backend/asr/socket_handlers.py

import asyncio
import numpy as np

from backend.sio import sio
from backend.db.asr_db import save_log_to_db
from backend.asr.managers.model_manager import model_manager

# sid ë³„ SpeechRecognizer ë° done_future ì €ì¥
recognizers = {}

# Whisper / HuggingFaceìš© ë¡œì»¬ ëª¨ë¸ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜
@sio.on('start_transcribe')
async def start_transcribe(sid, data):
    print(f"[DEBUG] â–¶ start_transcribe called: sid={sid}, data={data}")
    model_id = data.get("model_id")

    if model_id not in model_manager.models:
        return await sio.emit('transcript', {'text': 'âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, room=sid)
    
    model = model_manager.models[model_id]["instance"]
    if model is None:
        await sio.emit('transcript', {'text': 'âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}, to=sid)
        return
    
    await sio.save_session(sid, {'model_id': model_id})

    await sio.emit('transcript', {'text': 'ğŸ™ ì „ì‚¬ ì¤€ë¹„ ì™„ë£Œ'}, to=sid)

@sio.on('audio_chunk')
async def audio_chunk(sid, data):
    session = await sio.get_session(sid)
    model_id = session.get("model_id")

    if not model_id or model_id not in model_manager.models:
        await sio.emit('transcript', {'text': 'âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸ì…ë‹ˆë‹¤.'},  to=sid)
        return
    
    try:
        audio_np = np.array(data, dtype=np.float32)

        texts = model_manager.infer(model_id, audio_np, language="<|ko|>")
        # print("[DEBUG] ì „ì‚¬ ê²°ê³¼: ", texts)
        if texts:
            await sio.emit('transcript', {'text': texts[0]}, to=sid)
    except Exception as e:
        # print(f"[ERROR] audio_chunk ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        await sio.emit('transcript', {'text': 'âŒ ì „ì‚¬ ì‹¤íŒ¨'}, to=sid)

@sio.on('stop_transcribe')
async def stop_transcribe(sid):
    print(f'[SOCKET] stop_transcribe ìš”ì²­ ë°›ìŒ from {sid}')