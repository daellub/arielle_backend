# backend/llm/emotion/analyzer.py
import httpx
from typing import Dict
from backend.llm.emotion.generator import generate_prompt
from backend.llm.emotion.extractor import extract_emotion_json

# ğŸ“Œ ì‹¤ì œ llama.cpp ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ IPë¡œ ê³ ì •
LLAMA_ENDPOINT = "http://host.docker.internal:8081/v1/completions"

async def analyze_emotion(text: str) -> Dict[str, str]:
    try:
        prompt = generate_prompt(text)
        # print("âœ… generate_prompt ì„±ê³µ:", prompt)
    except Exception as e:
        # print("âŒ generate_prompt ì‹¤íŒ¨:", e)
        raise
    payload = {
        "model": "emotion-analyzer",
        "prompt": prompt,
        "temperature": 0.2,
        "max_tokens": 64,
        "stream": False
    }

    # print("\n[ğŸ§  ê°ì • ë¶„ì„ ì‹œì‘]")
    # print("ğŸ“¨ ì…ë ¥ í…ìŠ¤íŠ¸:", text)
    # print("ğŸ“¤ ì „ì†¡ í”„ë¡¬í”„íŠ¸:\n", prompt)
    # print("ğŸ“¦ Payload:", payload)

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            res = await client.post(LLAMA_ENDPOINT, json=payload)
            res.raise_for_status()
            content = res.json()["choices"][0]["text"].strip()
            print("ğŸ“¥ LLM ì‘ë‹µ ì›ë¬¸:\n", content)
    except httpx.RequestError as e:
        print("âŒ RequestError:", e)
        raise ValueError(f"Request failed: {e}")
    except httpx.TimeoutException:
        print("âŒ TimeoutException")
        raise ValueError("The request timed out after 60 seconds.")
    except httpx.HTTPStatusError as e:
        print("âŒ HTTPStatusError:", e.response.status_code)
        raise ValueError(f"HTTP error occurred: {e.response.status_code}")

    try:
        parsed = extract_emotion_json(content)
        print("âœ… íŒŒì‹±ëœ ê²°ê³¼:", parsed)
        return parsed
    except Exception as e:
        print("âŒ íŒŒì‹± ì‹¤íŒ¨:", e)
        print("â“ ì›ë¬¸ ì‘ë‹µ ë‹¤ì‹œ ì¶œë ¥:", repr(content))
        raise ValueError(f"ê°ì • ë¶„ì„ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
